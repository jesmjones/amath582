{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_jj_functions.ipynb","provenance":[],"authorship_tag":"ABX9TyPsCHnpKEWnodCxx0+2LUKa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MZeA0MdYgkDR"},"outputs":[],"source":["def cv_comparison(models, X, y, cv):\n","    # Initiate a DataFrame for the averages and a list for all measures\n","    cv_accuracies = pd.DataFrame()\n","    maes = []\n","    mses = []\n","    r2s = []\n","    accs = []\n","    # Loop through the models, run a CV, add the average scores to the DataFrame and the scores of \n","    # all CVs to the list\n","    for model in models:\n","        mae = -np.round(cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv), 4)\n","        maes.append(mae)\n","        mae_avg = round(mae.mean(), 4)\n","        mse = -np.round(cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv), 4)\n","        mses.append(mse)\n","        mse_avg = round(mse.mean(), 4)\n","        r2 = np.round(cross_val_score(model, X, y, scoring='r2', cv=cv), 4)\n","        r2s.append(r2)\n","        r2_avg = round(r2.mean(), 4)\n","        acc = np.round((100 - (100 * (mae * len(X))) / sum(y)), 4)\n","        accs.append(acc)\n","        acc_avg = round(acc.mean(), 4)\n","        cv_accuracies[str(model)] = [mae_avg, mse_avg, r2_avg, acc_avg]\n","    cv_accuracies.index = ['Mean Absolute Error', 'Mean Squared Error', 'R^2', 'Accuracy']\n","    return cv_accuracies, maes, mses, r2s, accs\n"]}]}